{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import csv\n",
        "\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "Qiu4Y8mAYQgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3E6Lk9pXYM0B"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def keep_first_two_sentences(text, lang):\n",
        "    sentences = sent_tokenize(text, language=lang)\n",
        "    return \" \".join(sentences[:2])\n",
        "\n",
        "# Set file paths (example for German)\n",
        "lang_code    = \"de\"                                    # \"en\", \"de\", or \"ru\"\n",
        "input_file   = f\"wikidata5m_top200_{lang_code}_42k_descriptions.tsv\"\n",
        "output_file  = f\"wikidata5m_top200_{lang_code}_42k_descriptions_truncated.tsv\"\n",
        "\n",
        "language_map = {\"en\": \"english\", \"de\": \"german\", \"ru\": \"russian\"}\n",
        "token_lang   = language_map[lang_code]\n",
        "\n",
        "with open(input_file, encoding=\"utf-8\") as infile, \\\n",
        "     open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as outfile:\n",
        "\n",
        "    reader = csv.reader(infile, delimiter=\"\\t\")\n",
        "    writer = csv.writer(outfile, delimiter=\"\\t\")\n",
        "\n",
        "    header = next(reader)\n",
        "    writer.writerow(header)\n",
        "\n",
        "    for row in reader:\n",
        "        if len(row) != 8:\n",
        "            continue                                   # skip malformed rows\n",
        "        subj_desc = keep_first_two_sentences(row[6], token_lang)\n",
        "        obj_desc  = keep_first_two_sentences(row[7], token_lang)\n",
        "        writer.writerow(row[:6] + [subj_desc, obj_desc])\n",
        "\n",
        "print(\"Truncated dataset saved:\", output_file)\n"
      ]
    }
  ]
}